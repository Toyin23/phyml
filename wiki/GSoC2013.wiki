#labels Featured
=Extend PhyML to use the BEAGLE library=

Much of the speed in PhyML comes from heuristic search techniques to efficiently search the space of possible trees. Parallelization is limited to MPI-based distribution of bootstrap replicates. Like most such software it uses the Felsenstein Pruning algorithm to compute the likelihood of individual trees using continuous-time Markov models. Recently considerable advances has been made in the fine-scale parallelization of this algorithm, in particular targeting massively parallel hardware such as NVidia GPGPUs (general purpose graphics processing units). There are considerable improvements in overall speed to be gained for PhyML by combining efficient search strategies with high speed likelihood computation.

BEAGLE is a cross-platform library that implements Felsenstein algorithm on a range of parallel and vector hardware including CUDA-based GPGPUs and SSE instructions on Intel chips:

http://code.google.com/p/beagle-lib/

This project involves extending PhyML to make calls to the BEAGLE library in place of the internal likelihood calculations. The project will involve: 1) Becoming familiar with the API of BEAGLE and linking of client software to the library (an example simple client in C is available in the BEAGLE package). 2) Understanding the likelihood calculations in PhyML. 3) Replacing likelihood calculation calls in PhyML with homologous calls to BEAGLE. 4) Testing and validation. 5) Performance testing with different hardware.

_This project was first proposed by Andrew Rambaut and listed by NESCENT among the proposals offered to students for GSoC2013. Imran Fanaswala's application was successful. Andrew Rambaut, Marc Suchard and Stephane Guindon will supervise Imran's work._

==Week1==
  *  In PhyML, Replace Update_P_Lk() with homologous BEAGLE call(s). Essentially this means PhyML will use BEAGLE for its partial likelihood calculation.
  * Figure out why the spr.c is throwing an error
  * How large (if any) is the performance gain from SetTransitionMatrices() versus SetTransitionMatrix(). The latter is the "first step", but if BEAGLE authors claim the former is faster then maybe its worth juggling the code in order to use it?

==Week2==
  * Likelihood calculations for a fixed tree (i.e. no parameter optimization). But...
  * For the toy.txt dataset... in PhyML, the LEFT subtree of Branch2 and the LEFT subtree of Branch3 both have the same partials? In BEAGLE they have different partials. Yet both final likelihoods match!?
  * FIXME: Estimating supports crashes
  * FIXME: Estimating parameter crashes
  * I think the above are interlated to incorrect partials in Lk_Core(). Which is strange, because that bit of code is oblivious to the "external changes" (or is it?). Could it be that p_lk_left/right are being indexed somewhere else too?

==Week3==
  * Use run_datasets.py to test (not benchmark!) for correctness
    * AA and DNA datasets
    * SPR, NNI, BEST
    * various other parameters
    * single and double precision 
    * Ambiguous characters
    * Mixed Models

==Update/Issues==

  # First of all, keep in mind that the scaling is manually done per Stephane's suggestion (as it is quite subtle from what I've been told). Though in my latest push (PhyML's beagle branch), I have *disabled* the scaling code completely for PhyML (i.e. in `Update_P_Lk()` ) and in the BEAGLE interface (i.e. in `update_partials_beagle()`). Why, you ask?
    * In the BEAGLE interface (i.e. `update_partials_beagle()` ), the `curr_scaler` hits 4294967296 (2^32), but this does NOT happen in the PhyML (i.e. `Update_P_Lk()`)... eventhogh the actual code is the same. ( Of course, I only found this out on a non-trivial dataset while diffing a large memory dump... but anyway )
    * For the issue I will explain below, disabling the scaling seems to have no effect... so I try to reduce the confounding factors and disable it. So lets proceed, shall we? ok...
  # For the toy dataset, and a *fixed* tree (i.e. no ratio tests, no parameter optimizations), I get the exact same final likelihoods in PhyML and BEAGLE. Good! However, for a larger dataset, I get slightly different likelihoods. Specifically, the following two commands yield different final likelihoods:
        `./src/phyml-beagle -i ./datasets/17.codon.paml -d nt -q -c 4 -v 0 -t e -m JC69 -f '0.25,0.25,0.25,0.25' -o none -b 0 --r_seed 1999`
        Final Lk: -17709.873912
        `./src/phyml             -i ./datasets/17.codon.paml -d nt -q -c 4 -v 0 -t e -m JC69 -f '0.25,0.25,0.25,0.25' -o none -b 0 --r_seed 1999`
        Final Lk: -17690.439032
    * I have artificially replaced gaps in the toy dataset and 17.codon.paml datasets (i.e. there are no ambigious characters)
    * Both datasets have a "crunched" sequence
    * Recall that the scaling code has been disabled, so it cant be that, right?
    * Recall that BEAGLE is supplied the P-Matrices (via SetTransitionMatrix()) for each branch. Thus the issue of rates doesn't even arise.
    * So thus, the BEAGLE callchain is SetTransitionMatrix() --> UpdatePartials() --> GetPartials() .. thats it. So... why are the final likelihoods different for the 17.codon.paml dataset? any ideas?
  # Of course, I investigated the above question myself... I compared the P-matrices and the Partials on each edge. What did I find? Remember earlier I told you that the toy dataset gave the exact same final likelihoods? This is true. But, when I print the partials on each edge ... I notice that the partials on "left subtree of Branch 2" are different in PhyML and BEAGLE. In PhyML, the partials at the "left subtree" of Branch2 and Branch3 are the same... but in BEAGLE they are different. Yet... I still get the same final likelihood!
  # Is it possible that I am simply just using BEAGLE incorrectly? [http://codepad.org/ZyOniPkP Here] you can see the PhyML partial likelihood function and its BEAGLE homolog. Correct me if I am wrong, PhyML stores partials and P-matrices on the Edges rather than on the Nodes. In other words, each Edge struct has a `edge->p_lk_left/right` vector representing the partials on the left and right subtree respectively and a `edge->Pij` matrix (of dimension rate*state*state). PhyML then calls Update_P_Lk(d,b) which "updates partial likelihood on edge b on the side of b where node d lies". Ok so far so good? Next, I create a homolog BEAGLE function update_partial_pk(d,b) which does the same thing with the operation: `BeagleOperation operations[1] = {{d->num, BEAGLE_OP_NONE, BEAGLE_OP_NONE, n_v1->num, b1->num, n_v2->num, b2->num}};` Observe that the *child partials are indexed by the nodes(i.e. n_v1, n_v2) but the child transition matrixes are indexed by the edges (i.e. b1, b2)*. Does this make sense?
  # A follow up... partials computed in `calcPartialsPartials()` don't match PhyML. Is this a hint?

===QUESTIONS===
  * Currently, I am providing the PMat to BEAGLE (via beagleSetTransitionMatrix()) in the Update_PMat_At_Given_Edge() because thats where the PMat is created. However, there are several other places where the PMat() is also called; for example, in M4_Integral_Term_On_One_Edge() and even Print_Model(). What shall I do there?

S: Update_PMat_At_Given_Edge() is the right place to call beagleSetTransitionMatrix(). You don't need to worry about the cases where PhyML calls PMat() directly (these parts of PhyML are deprecated).

  * Am I using beagleCalculateEdgeLogLikelihoods() correctly? <insert picture>
  * In beagleCalculateEdgeLogLikelihoods(), what are category weights parameter? do I need it for PhyML?
  * Is it safe to set to call SetPatternWeights(), SetRates(), etc eventho I am directly setting the Transition Prob matrix with SetTransitionMatrix()
  * Does Pij_rr hold the probabilities, first, and second derivatives? IOW, it is of dimension statecount*state*count*3 ? correct?

S: Pij_rr holds the transition probabilities for each rate category. Its dimension is therefore mod->ns * mod->ns * mod->ras->n_catg

  * What is compactBufferCount? How can I determine this?
  * Why does struct __EquFreq have a *next and *prev pointers?

S: the *next and *prev pointers are here to connect to equilibrium frequency vectors that apply to other elements of a data partition (multi-gene analysis).

  * In PhyML, are the substitution rates in ras->gamma_rr->v ? If so, it seems the gamma_rr->len is misleading as it is unused; shall I remove it?

S: I'll first take a look at it and will get back to you.

  * Also on a slightly related note, I ran PhyML with parameters "-m GTR -o tlr"... and noticed (via Print_Model()) that, instead of the default 4 rates, 2 rates were printed? is PhyML overriding the user's preference and modifying mod->ras->n_catg ?

S: In the first stages of the tree topology estimation, we use 2 classes when the user-required number of classes is > 2 in order to speed up the calculations. We switch back to the required number afterwards.

  * PhyML uses "phydbl" throughout which can either be "float" or "double". However, BEAGLE uses "double" for function arguments -- I suspect I will need to do "float to double" conversion? Any caveats I should be aware of here?

S: I used simple precision to run some tests, which where rather inconclusive if I remember well. You could just disable compiling PhyML with BEAGLE when phydbl is a float.

  * Eventho I only use SetTransitionMatrix(), I cant use eigenBufferCount=0 in createBeagleInstance(). Is this expected behavior? If not, I can attempt a fix?
  * PhyML; Is io->do_alias_subpatt ever TRUE in main()? IOW, when is the code at line 187 ever executed?

S: it is executed if you provide the option --alias_subpatt om the command line.

  * If I understand correctly, each rate category will have *a* corresponding weight, right? I dont understand why beagleSetCategoryWeights() takes an array
  * PhyML; can PhyML accept Tips with partials?

S: the partials are here integers rather than floating values for the internal nodes.

  * PhyML also computes the imaginary part of the Eigen decomposition; how is this handled by BEAGLE?
  * compactBufferCount; when and why is it not the same as tipCount (i.e. number of taxa)?
  * Not sure what are "scalingBuffers", and how to determine them?
  * In the example code: int nodeIndices[4] =     { 0, 1, 2, 3 };
                       double edgeLengths[4] = { 0.1, 0.1, 0.2, 0.1 };
    Considering there is a 1-to-1 correspondance, can I say that two nodes (maybe 0 and 1) are referring to the *same* branch? (maybe the branch with edgeLength=0.1?)
  *  For PhyML; should I instantiate Beagle with SINGLE or DOUBLE precision? I believe PhyML uses double everywhere?

S: it does by default (i.e. when phydbl is double)

  * It seems the Eigen is only computed *and* updated for the HKY85, GTR, and Custom models. If so, mathematically, how does one compute the Pmatrix for the other models?

S: there are analytical formula available for some nucleotide substitution models. 

  * Unconstrained log-likelihood?

S: that's the likelihood calculated under a multinomial model which does not take into account the phylogeny.

  * Recall, the user specifies the number of rate categories and now suppose we estimate an alpha; so now we have a Gamma distribution. But now, how do we actually determine the 4 rates? do we simply randomly sample 4 times from the distribution?

S: the Gamma distibution is discretized, i.e., cut into pieces of equal probabilities. Relative rates are then chosen as the 'middle' (i.e., the median or the mean) of each such piece. Note however that the probabilities for the different classes may not always be equal (I am currently implementing a model where both rates and frequencies are directly estimated from the data).
 