#labels Featured
=Extend PhyML to use the BEAGLE library=

Much of the speed in PhyML comes from heuristic search techniques to efficiently search the space of possible trees. Parallelization is limited to MPI-based distribution of bootstrap replicates. Like most such software it uses the Felsenstein Pruning algorithm to compute the likelihood of individual trees using continuous-time Markov models. Recently considerable advances has been made in the fine-scale parallelization of this algorithm, in particular targeting massively parallel hardware such as NVidia GPGPUs (general purpose graphics processing units). There are considerable improvements in overall speed to be gained for PhyML by combining efficient search strategies with high speed likelihood computation.

BEAGLE is a cross-platform library that implements Felsenstein algorithm on a range of parallel and vector hardware including CUDA-based GPGPUs and SSE instructions on Intel chips:

http://code.google.com/p/beagle-lib/

This project involves extending PhyML to make calls to the BEAGLE library in place of the internal likelihood calculations. The project will involve: 1) Becoming familiar with the API of BEAGLE and linking of client software to the library (an example simple client in C is available in the BEAGLE package). 2) Understanding the likelihood calculations in PhyML. 3) Replacing likelihood calculation calls in PhyML with homologous calls to BEAGLE. 4) Testing and validation. 5) Performance testing with different hardware.

_This project was first proposed by Andrew Rambaut and listed by NESCENT among the proposals offered to students for GSoC2013. Imran Fanaswala's application was successful. Andrew Rambaut, Marc Suchard and Stephane Guindon will supervise Imran's work._

==Update1==
  *  In PhyML, Replace Update_P_Lk() with homologous BEAGLE call(s). Essentially this means PhyML will use BEAGLE for its partial likelihood calculation.
  * Figured out why the spr.c is throwing an error
  * How large (if any) is the performance gain from SetTransitionMatrices() versus SetTransitionMatrix(). The latter is the "first step", but if BEAGLE authors claim the former is faster then maybe its worth juggling the code in order to use it?

==Update2==
  * Identified and worked on several Issues ( PhyML: http://bit.ly/13ms2JN, BEAGLE: http://bit.ly/12A4wHg )
  * Initial values of Brent's algorithm were inconsistent across binaries. This resulted in an extra iteration between runs, which then triggered a premature cutoff. The "culprit" was ultimately the gcc flags `fomit-frame-pointer` and `funroll-loops`
  * Created an overload for aLRT() that (re)uses the existing in memory tree
  * Created project specific Valgrind suppression files and `.gdbinit` files. The latter facilitates invoking functions without touching the binary (since some of the issues were not reproducible). Additionally, PhyML has hierarchy of structs with many pointer fields, which thus requires "deep printing". This was done using gdb's Python scripting
  * Likelihood calculations work now for a user provided tree (w/ branch lengths). This didn't work earlier due to a incorrect tip-numbering (i.imgur.com/Rgwa6yh.png). BTW, PhyML-BEAGLE now uses `SetTipPartials()` instead of `SetTipStates()`
  * Work in progress on optimizing branch lengths, topology, and finally estimation branch supports.
  * Merged, and tested, with new changes from trunk.

==Update3==
  * (Re)Implemented a client side scheme for indexing the appropriate GPU buffer. Details: PhyML uses both post and pre order traversals, which essentially means we need twice the number of partial vectors. Moreover, we needed a client side scheme for indexing the appropriate vector
  * Branch supports and branch lengths are estimated
  * Worked on SPR and NNI. While pruning and grafting subtrees, the likelihood vectors are shifted around; these actions need corresponding BEAGLE operations. The easy/lazy solution would be to simply call `beagleSetPartials()`, but this would allocate new memory rather than juggle pointers. The aforementioned indexing scheme, after a couple of revision, now cleanly allows this. 


==Update4==
  * Previously, while optimizing topology we created two BEAGLE instances (with different rate categories) and discarded the first one after recording the topology. But now, we only use one instance with all but set the weights of the "unnecessary" categories to 0.0
  * As of now, all partial likelihoods are computed with BEAGLE for all combinations of parameters (i.e. branch length, topology, ts/tv ratio, rates, etc)
  * Code cleanup and commenting

==Update5==
  * Enabled rescaling of partials (not so efficient yet; work-in-progress)
  * Merged with SVN trunk
  * Re-read some literature

==Update6==
  * Tested and achieved bit-wise compatability on AA datasets
  * Update batch test script
  * Discussion about scaling implementation. Relegated for the time being

==Update 7==
  * On a fixed tree, P-matrix is computed by BEAGLE using UpdateTransitionMatricies()/SetEigenDecomposition()
  * run_datasets.py now extracts and appends execution timings to the same logfile

==Next Steps==
  * P-Matrix computation with Beagle
    * Investigate `Simu_Loop()`, `Round_Optimize()`, and other code paths which optimize branch lengths and rates. Ultimately, we want the P-matrix to be be computed by BEAGLE while optimziing parameters even.
    * Pending cleanup of debugging statements in PhyML and BEAGLE
    * Handle AA models
    * Decide if its better to log() the eigen-values before sending to BEAGLE; or, prevent exp()ing of the eigen-values via a conditional compilation block. The latter sounds better, but there are cases where we want exp()ed eigen-values even with BEAGLE enabled in order to behave well with the rest of the system
    * Investigate PMat_Zero_br_Len()
    * How will capping the rate categories (to 2) behave now? Investigate this
    * Handle the Warn_And_Exit() cases
    * Figure out a way to cleanly(and safely!) Free() the heap allocated vectors in the 3 places in create_beagle_instance()
    * mod->ras->gamma_rr->len still misleading...
    * Test with script
    * May not need to call `update_beagle_ras()` and `update_beagle_eigen()` in `Update_RAS()` and `Update_Eigen()`? Since you are calling them in `Update_PMat_On_Edge()` anyway
  * Test with various hardware, rooted trees, custom model, various hardware
    * AA and DNA datasets
    * SPR, NNI, BEST
    * various other parameters
    * single and double precision 
  * (Re)enable scaling and test
  * Make run_datasets.py post-process the timings
  * Benchmark
  * Merge into trunk
  * Update PhyML documentation

===OLD NOTES===
  # First of all, keep in mind that the scaling is manually done per Stephane's suggestion (as it is quite subtle from what I've been told). Though in my latest push (PhyML's beagle branch), I have *disabled* the scaling code completely for PhyML (i.e. in `Update_P_Lk()` ) and in the BEAGLE interface (i.e. in `update_partials_beagle()`). Why, you ask?
    * In the BEAGLE interface (i.e. `update_partials_beagle()` ), the `curr_scaler` hits 4294967296 (2^32), but this does NOT happen in the PhyML (i.e. `Update_P_Lk()`)... eventhogh the actual code is the same. ( Of course, I only found this out on a non-trivial dataset while diffing a large memory dump... but anyway )
    * For the issue I will explain below, disabling the scaling seems to have no effect... so I try to reduce the confounding factors and disable it. So lets proceed, shall we? ok...
  # For the toy dataset, and a *fixed* tree (i.e. no ratio tests, no parameter optimizations), I get the exact same final likelihoods in PhyML and BEAGLE. Good! However, for a larger dataset, I get slightly different likelihoods. Specifically, the following two commands yield different final likelihoods:
        `./src/phyml-beagle -i ./datasets/17.codon.paml -d nt -q -c 4 -v 0 -t e -m JC69 -f '0.25,0.25,0.25,0.25' -o none -b 0 --r_seed 1999`
        Final Lk: -17709.873912
        `./src/phyml             -i ./datasets/17.codon.paml -d nt -q -c 4 -v 0 -t e -m JC69 -f '0.25,0.25,0.25,0.25' -o none -b 0 --r_seed 1999`
        Final Lk: -17690.439032
    * I have artificially replaced gaps in the toy dataset and 17.codon.paml datasets (i.e. there are no ambigious characters)
    * Both datasets have a "crunched" sequence
    * Recall that the scaling code has been disabled, so it cant be that, right?
    * Recall that BEAGLE is supplied the P-Matrices (via SetTransitionMatrix()) for each branch. Thus the issue of rates doesn't even arise.
    * So thus, the BEAGLE callchain is SetTransitionMatrix() --> UpdatePartials() --> GetPartials() .. thats it. So... why are the final likelihoods different for the 17.codon.paml dataset? any ideas?
  # Of course, I investigated the above question myself... I compared the P-matrices and the Partials on each edge. What did I find? Remember earlier I told you that the toy dataset gave the exact same final likelihoods? This is true. But, when I print the partials on each edge ... I notice that the partials on "left subtree of Branch 2" are different in PhyML and BEAGLE. In PhyML, the partials at the "left subtree" of Branch2 and Branch3 are the same... but in BEAGLE they are different. Yet... I still get the same final likelihood!
  # Is it possible that I am simply just using BEAGLE incorrectly? [http://codepad.org/ZyOniPkP Here] you can see the PhyML partial likelihood function and its BEAGLE homolog. Correct me if I am wrong, PhyML stores partials and P-matrices on the Edges rather than on the Nodes. In other words, each Edge struct has a `edge->p_lk_left/right` vector representing the partials on the left and right subtree respectively and a `edge->Pij` matrix (of dimension rate*state*state). PhyML then calls Update_P_Lk(d,b) which "updates partial likelihood on edge b on the side of b where node d lies". Ok so far so good? Next, I create a homolog BEAGLE function update_partial_pk(d,b) which does the same thing with the operation: `BeagleOperation operations[1] = {{d->num, BEAGLE_OP_NONE, BEAGLE_OP_NONE, n_v1->num, b1->num, n_v2->num, b2->num}};` Observe that the *child partials are indexed by the nodes(i.e. n_v1, n_v2) but the child transition matrixes are indexed by the edges (i.e. b1, b2)*. Does this make sense?
  # A follow up... partials computed in `calcPartialsPartials()` don't match PhyML. Is this a hint?

  * Currently, I am providing the PMat to BEAGLE (via beagleSetTransitionMatrix()) in the Update_PMat_At_Given_Edge() because thats where the PMat is created. However, there are several other places where the PMat() is also called; for example, in M4_Integral_Term_On_One_Edge() and even Print_Model(). What shall I do there?

S: Update_PMat_At_Given_Edge() is the right place to call beagleSetTransitionMatrix(). You don't need to worry about the cases where PhyML calls PMat() directly (these parts of PhyML are deprecated).

  * Does Pij_rr hold the probabilities, first, and second derivatives? IOW, it is of dimension statecount*state*count*3 ? correct?

S: Pij_rr holds the transition probabilities for each rate category. Its dimension is therefore mod->ns * mod->ns * mod->ras->n_catg

  * Why does struct __EquFreq have a *next and *prev pointers?

S: the *next and *prev pointers are here to connect to equilibrium frequency vectors that apply to other elements of a data partition (multi-gene analysis).

  * In PhyML, are the substitution rates in ras->gamma_rr->v ? If so, it seems the gamma_rr->len is misleading as it is unused; shall I remove it?

S: I'll first take a look at it and will get back to you.

  * Also on a slightly related note, I ran PhyML with parameters "-m GTR -o tlr"... and noticed (via Print_Model()) that, instead of the default 4 rates, 2 rates were printed? is PhyML overriding the user's preference and modifying mod->ras->n_catg ?

S: In the first stages of the tree topology estimation, we use 2 classes when the user-required number of classes is > 2 in order to speed up the calculations. We switch back to the required number afterwards.

  * PhyML uses "phydbl" throughout which can either be "float" or "double". However, BEAGLE uses "double" for function arguments -- I suspect I will need to do "float to double" conversion? Any caveats I should be aware of here?

S: I used simple precision to run some tests, which where rather inconclusive if I remember well. You could just disable compiling PhyML with BEAGLE when phydbl is a float.

  * PhyML; Is io->do_alias_subpatt ever TRUE in main()? IOW, when is the code at line 187 ever executed?

S: it is executed if you provide the option --alias_subpatt om the command line.

  * PhyML; can PhyML accept Tips with partials?

S: the partials are here integers rather than floating values for the internal nodes.

  *  For PhyML; should I instantiate Beagle with SINGLE or DOUBLE precision? I believe PhyML uses double everywhere?

S: it does by default (i.e. when phydbl is double)

  * It seems the Eigen is only computed *and* updated for the HKY85, GTR, and Custom models. If so, mathematically, how does one compute the Pmatrix for the other models?

S: there are analytical formula available for some nucleotide substitution models. 

  * Unconstrained log-likelihood?

S: that's the likelihood calculated under a multinomial model which does not take into account the phylogeny.

  * Recall, the user specifies the number of rate categories and now suppose we estimate an alpha; so now we have a Gamma distribution. But now, how do we actually determine the 4 rates? do we simply randomly sample 4 times from the distribution?

S: the Gamma distibution is discretized, i.e., cut into pieces of equal probabilities. Relative rates are then chosen as the 'middle' (i.e., the median or the mean) of each such piece. Note however that the probabilities for the different classes may not always be equal (I am currently implementing a model where both rates and frequencies are directly estimated from the data).
 